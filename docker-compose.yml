services:
  app:
    image: ghcr.io/kiquetal/ai-rag-poc:20251116170716-amd64
    ports:
      - "8082:8082"
    environment:
      - INFINISPAN_HOSTS=infinispan:11222
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GEMINI_PROJECT_ID=${GEMINI_PROJECT_ID}
      - INFINISPAN_USER=admin
      - INFINISPAN_PASS=password
    depends_on:
      infinispan:
        condition: service_healthy
    networks:
      - rag-network

  infinispan:
    image: infinispan/server:15.1
    ports:
      - "11222:11222"
    environment:
      - USER=admin
      - PASS=password
    networks:
      - rag-network
    healthcheck:
      test: ["CMD", "/opt/infinispan/bin/probe.sh"]
      interval: 10s       # Check every 10 seconds
      timeout: 5s         # Wait 5 seconds for the check to respond
      retries: 30         # Try up to 30 times (5 minutes total)
      start_period: 10s   # Give the container 10 seconds to start up before checking
  podman-ai-lab:
    image: quay.io/podman-desktop/ai-lab:latest
    ports:
      - "8088:8080"
      - "5000:5000"
    volumes:
      - ai-lab-data:/data
    networks:
      - rag-network

  granite:
    image: quay.io/ramalama/ramalama:latest
    container_name: granite-cpu-server

    # Expose the API port (OpenAI compatible)
    ports:
      - "8080:8080"

    # Mount your local models folder to /mnt/models inside the container
    volumes:
      - ./models:/mnt/models:ro

    environment:
      - HOST=0.0.0.0
      - PORT=8080
      - LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib64:/usr/lib64:${LD_LIBRARY_PATH}
      # Force CPU mode by hiding CUDA devices (optional but recommended for CPU-only)
      - CUDA_VISIBLE_DEVICES=-1

      # Volume Configuration:
      # Mounts the host's RamaLama model store into the container.
      # The ':ro' flag protects the model files from modification.
      # Source: [14, 11]

      # Command Override:
      # Bypasses the default entrypoint script to run the server directly.
      # Arguments are tuned for a standard CPU deployment.
      # - '--model': Must match the path inside /mnt/models
      # - '-ngl 0': Forces 0 GPU layers to prevent crashes on non-GPU hosts.
      # - '--ctx-size': Context window size (2048 is a safe default for 8GB-16GB RAM)
      # Source: [6, 15]
    command: >
      /usr/bin/llama-server
      --model /mnt/models/granite-3.3-8b-instruct-Q4_K_M.gguf
      --alias granite
      --ctx-size 2048
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 0
      --threads 8
      --cache-reuse 256
networks:
  rag-network:
    driver: bridge

volumes:
  ai-lab-data:
  llm-model-data: {}
