services:
  app:
    image: ghcr.io/kiquetal/ai-rag-poc:20251116170716-amd64
    ports:
      - "8082:8082"
    environment:
      - INFINISPAN_HOSTS=infinispan:11222
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GEMINI_PROJECT_ID=${GEMINI_PROJECT_ID}
      - INFINISPAN_USER=admin
      - INFINISPAN_PASS=password
    depends_on:
      infinispan:
        condition: service_healthy
    networks:
      - rag-network

  infinispan:
    image: infinispan/server:15.1
    ports:
      - "11222:11222"
    environment:
      - USER=admin
      - PASS=password
    networks:
      - rag-network
    healthcheck:
      test: ["CMD", "/opt/infinispan/bin/probe.sh"]
      interval: 10s       # Check every 10 seconds
      timeout: 5s         # Wait 5 seconds for the check to respond
      retries: 30         # Try up to 30 times (5 minutes total)
      start_period: 10s   # Give the container 10 seconds to start up before checking
  podman-ai-lab:
    image: quay.io/podman-desktop/ai-lab:latest
    ports:
      - "8088:8080"
      - "5000:5000"
    volumes:
      - ai-lab-data:/data
    networks:
      - rag-network

  granite:
    # Image Selection:
    # Use 'quay.io/ramalama/ramalama:latest' for general auto-detection (best effort).
    # Use 'quay.io/ramalama/cuda:latest' for NVIDIA specific optimization.
    # Use 'quay.io/ramalama/rocm:latest' for AMD.
    image: quay.io/ramalama/ramalama:latest
    container_name: ramalama-granite-server
    restart: unless-stopped


    # Port Mapping: Map container port 8080 to host port 8080
    ports:
      - "8080:8080"

    # Volume Mounts:
    # 1. Mount the models directory read-only (:ro) for security.
    # 2. Mount local time for log consistency.
    volumes:
      - models:/mnt/models
    # Environment Variables:
    # These can be used by the entrypoint, but we will override the command
    # to be explicit.
    environment:
      - HOST=0.0.0.0
      - PORT=8080
      # Force generic images to look for specific hardware (optional)
      # - GGML_VK_VISIBLE_DEVICES=0

    # Hardware Acceleration (Uncomment the block matching your hardware)

    # --- Option A: NVIDIA GPU ---
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    # --- Option B: Intel/AMD (Device Mapping) ---
    # devices:
    #   - /dev/dri:/dev/dri
    #   - /dev/kfd:/dev/kfd  # For AMD specifically

    # Security Context:
    # Drop all capabilities and only add what is strictly necessary.
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true

    # The Execution Command:
    # Explicitly invoking 'llama-server' bypasses the shell entrypoint logic
    # to give us full declarative control over the inference parameters.
    command: >
      llama-server
      --model /mnt/models/granite-3.3-8b-instruct-Q4_K_M.gguf
      --alias granite-3.3-8b
      --host 0.0.0.0
      --port 8080
      --ctx-size 8192
      --n-gpu-layers 99
      --parallel 2
      --cont-batching
      --log-disable

    # Optional: Vector Database for RAG (Future Proofing)
    # qdrant:
    #   image: qdrant/qdrant
    #   ports: ["6333:6333"]
    #   networks: ["ramalama-net"]

networks:
  rag-network:
    driver: bridge

volumes:
  ai-lab-data:
  llm-model-data: {}
